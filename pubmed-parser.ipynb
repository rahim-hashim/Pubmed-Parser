{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed Parser\n",
    "### Textual Analysis\n",
    "\n",
    "__Notebook Description:__ PubmedParser is a Python-based web scraper specifically designed to scrape, parse, and analyze NCBI's online pubmed database (https://www.ncbi.nlm.nih.gov/pubmed). This scraper can be used in a number of ways, one of which is utilized here to performa a textual analysis of specific researchers' abstracts. <br>\n",
    "__Notebook Owner:__ Rahim Hashim <br>\n",
    "__Contact:__ rh2898@columbia.edu <br>\n",
    "__\\*Dependencies:__ In order to reproduce the environment used in the notebook, activate your virtual environment with the *environment.yml* file contained in the directory (see: [Managing Virtual Environments](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-from-an-environment-yml-file) for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "__Public List:__ os, re, sys, string, datetime, pandas, numpy, tqdm, ntlk, collections, urllib, bs4, unidecode<br>\n",
    "__Additional Code:__ Regions\n",
    "\n",
    "After following the directions for Managing Virtual Environments above, all of the libraries from the public list will be set up. Regions.py included in the directory will provide the rest of the functions used in this codebase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rahimhashim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahimhashim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "# For download info / documentation on Natural Language Toolkit (nltk):\n",
    "#    https://www.nltk.org/\n",
    "import nltk\n",
    "nltk.download('punkt') # only needs to be downloaded once\n",
    "nltk.download('stopwords') # only needs to be downloaded once\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Search Term Parameters\n",
    "\n",
    "First we will assign the search parameters for scraping. In particular, test the key terms you will be searcing for in the database of choice, and then assign it to the SearchParameters.searchTerms attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchObjects import SearchParameters\n",
    "\n",
    "parameters = {}\n",
    "# Database : Specified NCBI database\n",
    "#   Options = Pubmed [pubmed] | Pubmed Central [PMC] | Unigene [Unigene] | Others [Look Up Key]\n",
    "parameters['database'] = 'pubmed'\n",
    "# SearchTerms : PubMed desired search term(s)\n",
    "parameters['searchTerms'] = ['Salzman CD', 'Fusi S']\n",
    "# searchLimit : Number of articles that the program will search through for each search term\n",
    "parameters['searchLimit'] = 100\n",
    "# startIndex : Index of which article to start search on (e.g. IdIndex = 10 does not include 9 most recent articles published from SearchTerms)\n",
    "parameters['startIndex'] = 0\n",
    "# abstractFlag : Flag to hide (0) or capture (1) abstract info in last column\n",
    "parameters['abstractFlag'] = 1\n",
    "# emailFilter : Filter for contact-capturing - (0) for all results, (1) for articles with author emails only \n",
    "parameters['emailFilter'] = 0\n",
    "# geographyFilter : Filter to show results only from specified [country1, country2...] or keep all countries ([])\n",
    "# i.e. --> geographyFilter = ['US', 'USA', 'United States', 'United States of America']\n",
    "parameters['geographyFilter'] = []\n",
    "#authorTermSearch : When search term is author name, (1) results match last name of search terms only or (2) all authors \n",
    "parameters['authorTermSearch'] = 2\n",
    "# authorScore = Flag to keep off (0) or turn on (1) author score column\n",
    "parameters['authorScoreFlag'] = 0\n",
    "\n",
    "searchParameters = SearchParameters(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Generating List of Database Search Result URLs\n",
    "\n",
    "Using the [NCBI Entrez API](https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch), eSearchLinkGenerator generates an XML containing the list of URLs for the articles returned by searchParameters.searchTerms attribute, up until the amount of articles specified by searchLimit. resultsList generates a nested list of all article URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Entrez XML...\n",
      "   [Salzman CD] complete\n",
      "   [Fusi S] complete\n",
      "\n",
      "Generating list of PMIDs...\n",
      "  Salzman CD[Author]: 35 results\n",
      "  Fusi S[Author]: 100 results\n"
     ]
    }
   ],
   "source": [
    "from entrezParser import eSearchLinkGenerator, PMID_ListGenerator\n",
    "\n",
    "def entrezSearch():\n",
    "    '''\n",
    "    entrezSearch generates resultsList, which is\n",
    "    a list of all article URLs for each search term.\n",
    "    '''\n",
    "    eSearchCore = 'http://eutils.ncbi.nlm.nih.gov/entrez//eutils/esearch.fcgi/?db=&term=&retmax=&retstart='\n",
    "    eSearchLinkList = eSearchLinkGenerator(eSearchCore, searchParameters)\n",
    "    resultsList = PMID_ListGenerator(eSearchLinkList)\n",
    "    return(resultsList)\n",
    "\n",
    "resultsList = entrezSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Parsing Data\n",
    "\n",
    "For each searchTerm provided to searchParameters, dataParser will take each of the article URLs and parse the specified information, inserting it into a multi-nested dictionary queriesHash to be further analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing info for search terms...\n",
      "  Salzman CD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:28<00:00,  1.23it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fusi S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from linksParser import linksParser\n",
    "\n",
    "def dataParser(resultsList):\n",
    "    '''\n",
    "    dataParser creates a multi-nested dictionary\n",
    "      queriesHash\n",
    "        | \n",
    "        queriesHash[query]\n",
    "          |\n",
    "          queriesHash[query][PMID]\n",
    "            |\n",
    "            articleTitle\n",
    "            journalTitle\n",
    "            dataPublished\n",
    "            ...\n",
    "    '''\n",
    "    print('\\nParsing info for search terms...')\n",
    "    queriesHash = defaultdict(lambda: defaultdict(list)) # primary key = pubmed query\n",
    "    searchesHash = defaultdict(lambda: defaultdict(list)) # primary key = PMID\n",
    "    for a_index, termLinks in enumerate(resultsList):\n",
    "        query = searchParameters.searchTerms[a_index]\n",
    "        searchesHash = linksParser(a_index, termLinks, searchParameters,\n",
    "                                   query, searchesHash)\n",
    "        queriesHash[query] = searchesHash\n",
    "    return queriesHash\n",
    "\n",
    "searchesHash = dataParser(resultsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year-by-Year Analysis\n",
    "\n",
    "The first analysis we'll be performing is year-by-year lexicon frequency. In the example queries provided above, we introduced two authors (Salzman CD, Fusi S) and captured all meta-info (title, journal, date published, etc), and importantly all of the abstracts for each of the author's articles. \n",
    "\n",
    "In pre-processing we'll __remove stop words__, defined by [NLTK](https://www.nltk.org/book/ch02.html) as: \"high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearExaminer(searchesHash):\n",
    "    '''\n",
    "    yearExaminer buckets the results for each searchTerm \n",
    "    by year of publication and analyzes the lexicon used\n",
    "    in the abstracts.\n",
    "    \n",
    "    pre-processing: \n",
    "       1) lower-case for all words\n",
    "       2) tokenize to split words from punctuation\n",
    "       3) separates all punctuation to yearsHash[year]['punctuation'] (for sentence volume comparison)\n",
    "       4) separates all stop words to yearsHash[year]['wordsLeft']\n",
    "    '''\n",
    "    \n",
    "    stopWords = nltk.corpus.stopwords.words('english')\n",
    "    punctuation = string.punctuation\n",
    "    yearsHash = defaultdict(lambda: defaultdict(list))\n",
    "    for query in searchesHash.keys():\n",
    "        for date in searchesHash[query].keys():\n",
    "            year = searchesHash[query][date]['datePublished'][:4]\n",
    "            # word_tokenize splits off punctuation other than periods\n",
    "            abstract = nltk.tokenize.word_tokenize(searchesHash[query][date]['abstract'].lower())\n",
    "            for word in abstract:\n",
    "                if word in punctuation:\n",
    "                    yearsHash[year]['punctuation'].append(word)\n",
    "                elif word not in stopWords:\n",
    "                    yearsHash[year]['wordsKept'].append(word)\n",
    "                else:\n",
    "                    yearsHash[year]['wordsLeft'].append(word)\n",
    "    return(yearsHash)\n",
    "            \n",
    "yearsHash = yearExaminer(searchesHash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist_list = []\n",
    "for abstract in abstract_text:\n",
    "    abstract = [word for word in abstract if len(word) > 1]\n",
    "    abstract = [word.lower() for word in abstract]\n",
    "    stopWords = nltk.corpus.stopwords.words('english')\n",
    "    abstract = [word for word in abstract if word not in stopWords]\n",
    "    fdist = FreqDist(abstract)\n",
    "    fdist_list.append(FreqDist(abstract))\n",
    "    \n",
    "print('Total Abstracts Captured:', str(len(abstract_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "FreqDist.plot(fdist_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "total_freq_dict = {}\n",
    "for i in range(len(fdist_list)):\n",
    "    fdist_dict = Counter(dict(fdist_list[i]))\n",
    "    total_freq_dict_copy = Counter(total_freq_dict)\n",
    "    total_freq_dict = dict(fdist_dict + total_freq_dict_copy)\n",
    "total_freq_dict = OrderedDict(sorted(total_freq_dict.items(), reverse=True, key=lambda t: t[1]))\n",
    "print(total_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot Birth State of US-Born Players (2)\n",
    "f, ax = plt.subplots(1, figsize=(25,10))\n",
    "word_list = total_freq_dict.keys(); word_count = total_freq_dict.values()\n",
    "ax.bar(np.arange(len(word_list)), word_count)\n",
    "ax.set_xticks(np.arange(len(word_list)))\n",
    "ax.set_xticklabels(word_list, rotation=90, ha='right', fontsize=7)\n",
    "#ax.set_xlabel('US State of Birth', fontweight='bold', labelpad=10)\n",
    "#ax.set_ylabel('Number of Players', fontweight='bold', labelpad=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pubmed-parser",
   "language": "python",
   "name": "pubmed-parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
